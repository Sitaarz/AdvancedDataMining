{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "DATA_PATH = \"../../data/preprocessed_data.csv\""
   ],
   "id": "1399628502ff9a0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head()"
   ],
   "id": "114f9888afe5c9ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "bbb7c2c22a3bd104"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_stopwords(sentence: str)-> str:\n",
    "    if sentence is not str:\n",
    "        sentence = str(sentence)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [t for t in tokens if t not in set(stopwords.words('english'))]\n",
    "    return \" \".join(tokens)\n"
   ],
   "id": "cccc7f0df7e751af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = df.map(lambda x: str(x))",
   "id": "327ecc13e39c6fb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "e890086406332051",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lematizer = WordNetLemmatizer()\n",
    "\n",
    "def lematize(sentence: str) -> str:\n",
    "    lematizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lematized_tokens = [lematizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(lematized_tokens)\n",
    "\n",
    "df = df.map(lematize)"
   ],
   "id": "71edbb6a83bca71a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "6edfc5b8692ed469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embeddings",
   "id": "dc440708d18917c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Word2Vec",
   "id": "2af4fd8c11299205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gensim.downloader as api\n",
    "model_name = \"word2vec-google-news-300\"\n",
    "# api._download(model_name)"
   ],
   "id": "b7803e3174d41201",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "model = api.load(model_name)\n",
    "\n",
    "def get_avg_embedding(model, sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    word_vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.key_to_index:\n",
    "            word_vectors.append(model[token])\n",
    "\n",
    "    if word_vectors:\n",
    "        sentence_embedding = np.mean(word_vectors, axis=0)\n",
    "        return sentence_embedding\n",
    "    else:\n",
    "        print(\"Empty embedding. Generating random one.\")\n",
    "        return np.random.rand(300)\n",
    "\n",
    "title_word2vec = df['title'].apply(lambda sentence: get_avg_embedding(model, sentence))\n",
    "plot_word2vec = df['plot'].apply(lambda sentence: get_avg_embedding(model, sentence))\n",
    "director_word2vec = df['director'].apply(lambda sentence: get_avg_embedding(model, sentence))\n"
   ],
   "id": "ad2adb085c578c96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TF-IDF",
   "id": "1980add6accc0672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "title_TfIdf = vectorizer.fit_transform(df['title'])\n",
    "plot_TfIdf = vectorizer.fit_transform(df['plot'])\n",
    "director_TfIdf = vectorizer.fit_transform(df['director'])\n",
    "X_tfIdf = np.concatenate(\n",
    "    [\n",
    "    np.stack(title_TfIdf.toarray()),\n",
    "    np.stack(plot_TfIdf.toarray()),\n",
    "    np.stack(director_TfIdf.toarray())\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ],
   "id": "ac714d4719ac86fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoded_y = encoder.fit_transform(df['genre'].to_numpy().reshape((-1, 1)))\n",
    "y = np.array([np.argmax(number) for number in one_hot_encoded_y])\n",
    "\n",
    "# Zamiana np.concat na np.concatenate\n",
    "X_word2vec = np.concatenate(\n",
    "    [\n",
    "        np.stack(title_word2vec.values).reshape((-1, 300)),\n",
    "        np.stack(plot_word2vec.values).reshape((-1, 300)),\n",
    "        np.stack(director_word2vec.values).reshape((-1, 300))\n",
    "    ],\n",
    "    axis=1\n",
    ")\n"
   ],
   "id": "35a0dd019d49cfaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(X_word2vec.shape)\n",
    "print(y.shape)"
   ],
   "id": "ea0ffe97e5da4b6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6a712b725511128c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f85fb057ffc4dc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Logistic Regression",
   "id": "905ec04f72be261b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Word2Vec",
   "id": "4766569e4a22eef7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression(max_iter = 10_000)\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ],
   "id": "4da84b7152766ddb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix = conf_matrix)\n",
    "print(\"accuracy:\", accuracy)\n",
    "confusion_matrix_display.plot()\n"
   ],
   "id": "43c557c0ef50dad7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TF-IDF",
   "id": "5c1a49188a5902ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfIdf, y, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression(max_iter = 10_000)\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "5cbb1ed83ba82d78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred_tfIdf = model.predict(X_test)\n",
    "accuracy_tfIdf = accuracy_score(y_test, y_pred_tfIdf)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_tfIdf)\n",
    "confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix = conf_matrix)\n",
    "print(\"accuracy:\", accuracy_tfIdf)\n",
    "confusion_matrix_display.plot()"
   ],
   "id": "c44516e86442f94e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
